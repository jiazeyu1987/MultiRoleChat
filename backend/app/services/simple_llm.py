#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
简化的Anthropic LLM服务

基于 anthropic.Anthropic(api_key=None) 的最小LLM调用实现
无需设置API KEY，让SDK自动寻找认证方式

适配MultiRoleChat系统的简化LLM服务
"""

import anthropic
from typing import Optional, List, Dict, Any
import logging
import time
from dataclasses import dataclass

from ..utils.request_tracker import RequestTracker, log_llm_info, log_llm_error, log_llm_warning

logger = logging.getLogger(__name__)


@dataclass
class LLMResponse:
    """LLM响应数据结构"""
    content: str
    model: str
    usage: Dict[str, int]
    response_time: float
    provider: str = "anthropic"


class SimpleLLMService:
    """
    简化的LLM服务类

    基于用户提供的simple_anthropic_llm.py代码
    适配现有系统的接口需求
    """

    def __init__(self, api_key: Optional[str] = None):
        """
        初始化简化LLM服务

        Args:
            api_key (Optional[str]): API密钥，None表示自动寻找
        """
        self.api_key = api_key
        self.default_model = "claude-3-5-sonnet-20241022"
        self.default_max_tokens = 4096

        # 创建客户端
        self.client = anthropic.Anthropic(api_key=api_key)
        logger.info(f"简化LLM服务已初始化，模型: {self.default_model}")

    def llm_call(self, prompt: str) -> str:
        """
        最小的LLM调用函数

        Args:
            prompt (str): 输入提示词

        Returns:
            str: LLM响应内容
        """
        try:
            response = self.client.messages.create(
                model=self.default_model,
                max_tokens=self.default_max_tokens,
                messages=[{"role": "user", "content": prompt}]
            )
            return response.content[0].text

        except Exception as e:
            logger.error(f"LLM调用失败: {str(e)}")
            return f"调用失败: {str(e)}"

    def simple_llm(
        self,
        prompt: str,
        model: str = None,
        max_tokens: int = None
    ) -> str:
        """
        简单的LLM调用函数，基于用户的代码风格

        Args:
            prompt (str): 输入提示词
            model (str): 模型名称
            max_tokens (int): 最大输出token数

        Returns:
            str: LLM响应内容
        """
        try:
            model = model or self.default_model
            max_tokens = max_tokens or self.default_max_tokens

            messages = [{"role": "user", "content": prompt}]

            response = self.client.messages.create(
                model=model,
                max_tokens=max_tokens,
                messages=messages
            )

            if response.content:
                return response.content[0].text
            else:
                return "未收到有效响应"

        except anthropic.APIError as e:
            logger.error(f"API错误: {str(e)}")
            return f"API错误: {str(e)}"
        except Exception as e:
            logger.error(f"调用失败: {str(e)}")
            return f"调用失败: {str(e)}"

    def llm_call_with_history(
        self,
        prompt: str,
        history: Optional[List[Dict[str, str]]] = None,
        model: str = None,
        max_tokens: int = None
    ) -> str:
        """
        带历史记录的LLM调用函数

        Args:
            prompt (str): 当前用户输入
            history (Optional[List[Dict[str, str]]]): 对话历史
            model (str): 模型名称
            max_tokens (int): 最大输出token数

        Returns:
            str: LLM响应内容
        """
        try:
            model = model or self.default_model
            max_tokens = max_tokens or self.default_max_tokens

            messages = []

            # 添加历史消息
            if history:
                messages.extend(history)

            # 添加当前消息
            messages.append({"role": "user", "content": prompt})

            response = self.client.messages.create(
                model=model,
                max_tokens=max_tokens,
                messages=messages
            )

            if response.content:
                return response.content[0].text
            else:
                return "未收到有效响应"

        except anthropic.APIError as e:
            logger.error(f"API错误: {str(e)}")
            return f"API错误: {str(e)}"
        except Exception as e:
            logger.error(f"调用失败: {str(e)}")
            return f"调用失败: {str(e)}"

    def generate_response(
        self,
        messages: List[Dict[str, Any]],
        model: str = None,
        max_tokens: int = None,
        request_id: str = None,
        **kwargs
    ) -> LLMResponse:
        """
        生成LLM响应（适配现有系统接口）

        Args:
            messages (List[Dict[str, Any]]): 消息列表
            model (str): 模型名称
            max_tokens (int): 最大输出token数
            request_id (str): 请求ID用于日志追踪
            **kwargs: 其他参数

        Returns:
            LLMResponse: LLM响应对象
        """
        # 如果没有传入request_id，尝试从当前上下文获取
        if request_id is None:
            context = RequestTracker.get_context()
            request_id = context.request_id if context else "UNKNOWN"

        start_time = time.time()

        log_llm_info(
            "CORE",
            "开始生成Anthropic LLM响应",
            request_id,
            model=model or self.default_model,
            max_tokens=max_tokens or self.default_max_tokens,
            message_count=len(messages) if messages else 0,
            additional_params={k: v for k, v in kwargs.items() if k not in ['model', 'max_tokens', 'messages']}
        )

        try:
            model = model or self.default_model
            max_tokens = max_tokens or self.default_max_tokens

            # 转换消息格式
            anthropic_messages = []

            log_llm_info(
                "CORE",
                "开始转换消息格式为Anthropic API格式",
                request_id,
                input_format="字典列表",
                input_count=len(messages) if messages else 0,
                target_format="Anthropic API消息格式"
            )

            for i, msg in enumerate(messages):
                # 处理LLMMessage对象或字典
                if hasattr(msg, 'role'):
                    role = msg.role
                    content = msg.content
                else:
                    role = msg.get('role', 'user')
                    content = msg.get('content', str(msg))

                anthropic_message = {
                    "role": role,
                    "content": content
                }
                anthropic_messages.append(anthropic_message)

                log_llm_info(
                    "CORE",
                    f"消息格式转换[{i}] - {role}",
                    request_id,
                    content_length=len(content),
                    content_preview=content[:100] + "..." if len(content) > 100 else content,
                    conversion_success=True
                )

            log_llm_info(
                "CORE",
                "消息格式转换完成",
                request_id,
                output_format="Anthropic API消息列表",
                output_count=len(anthropic_messages),
                conversion_success=True
            )

            # 构建完整的提示词用于日志记录
            full_prompt = ""
            for msg in anthropic_messages:
                full_prompt += f"{msg['role']}: {msg['content']}\n"

            log_llm_info(
                "CORE",
                "准备调用Anthropic Claude API",
                request_id,
                content=full_prompt.strip(),
                total_prompt_length=len(full_prompt),
                api_start_time=start_time,
                api_model=model,
                api_max_tokens=max_tokens
            )

            # 记录API调用开始
            api_call_start = time.time()
            log_llm_info(
                "CORE",
                "调用Anthropic Claude API",
                request_id,
                api_endpoint="anthropic.messages.create",
                model=model,
                max_tokens=max_tokens,
                message_count=len(anthropic_messages),
                api_call_start_time=api_call_start
            )

            response = self.client.messages.create(
                model=model,
                max_tokens=max_tokens,
                messages=anthropic_messages,
                **kwargs
            )

            api_call_end = time.time()
            api_call_duration = api_call_end - api_call_start

            response_time = time.time() - start_time
            response_content = response.content[0].text if response.content else ""

            log_llm_info(
                "CORE",
                "Anthropic API调用成功",
                request_id,
                api_response_id=getattr(response, 'id', 'N/A'),
                api_call_duration=f"{api_call_duration:.3f}s",
                api_model=response.model,
                api_stop_reason=getattr(response, 'stop_reason', 'N/A'),
                response_content_preview=response_content[:100] + "..." if len(response_content) > 100 else response_content
            )

            # 估算token使用量
            prompt_text = " ".join([msg.get('content', '') for msg in messages])
            usage = {
                'prompt_tokens': len(prompt_text) // 4,
                'completion_tokens': len(response_content) // 4,
                'total_tokens': (len(prompt_text) + len(response_content)) // 4
            }

            # 检查响应质量
            is_api_success = bool(response_content and not response_content.startswith('调用失败'))
            if is_api_success:
                log_llm_info(
                    "CORE",
                    "Anthropic LLM响应生成成功",
                    request_id,
                    response_length=len(response_content),
                    response_time=f"{response_time:.3f}s",
                    api_call_time=f"{api_call_duration:.3f}s",
                    model=response.model,
                    provider="anthropic",
                    usage=usage,
                    is_success=True,
                    response_quality="正常"
                )

                # 记录完整的返回内容
                log_llm_info(
                    "CORE",
                    "Anthropic LLM返回内容详情",
                    request_id,
                    content=response_content,
                    content_length=len(response_content),
                    is_success=True,
                    response_source="anthropic_api"
                )
            else:
                log_llm_error(
                    "CORE",
                    "Anthropic LLM响应异常或为空",
                    request_id,
                    response_content=response_content,
                    response_time=f"{response_time:.3f}s",
                    api_call_time=f"{api_call_duration:.3f}s",
                    model=response.model,
                    provider="anthropic",
                    usage=usage,
                    is_success=False,
                    response_quality="异常"
                )

            llm_response = LLMResponse(
                content=response_content,
                model=model,
                usage=usage,
                response_time=response_time,
                provider="anthropic"
            )

            # 记录最终响应对象
            log_llm_info(
                "CORE",
                "LLMResponse对象构建完成",
                request_id,
                response_object_id=id(llm_response),
                response_content_length=len(llm_response.content),
                response_model=llm_response.model,
                response_provider=llm_response.provider,
                response_time=llm_response.response_time,
                total_processing_time=f"{response_time:.3f}s"
            )

            return llm_response

        except anthropic.APIError as e:
            end_time = time.time()
            total_time = end_time - start_time

            log_llm_error(
                "CORE",
                "Anthropic API错误",
                request_id,
                api_error=str(e),
                api_error_type=e.type if hasattr(e, 'type') else 'Unknown',
                api_status_code=e.status_code if hasattr(e, 'status_code') else 'Unknown',
                response_time=f"{total_time:.3f}s",
                error_details=str(e)
            )

            response_time = time.time() - start_time
            return LLMResponse(
                content=f"Anthropic API错误: {str(e)}",
                model=model or self.default_model,
                usage={'prompt_tokens': 0, 'completion_tokens': 0, 'total_tokens': 0},
                response_time=response_time,
                provider="anthropic"
            )

        except Exception as e:
            end_time = time.time()
            total_time = end_time - start_time

            log_llm_error(
                "CORE",
                "LLM响应生成异常",
                request_id,
                error=str(e),
                error_type=type(e).__name__,
                response_time=f"{total_time:.3f}s",
                model=model or self.default_model,
                max_tokens=max_tokens or self.default_max_tokens,
                stack_trace=f"异常位置: {__name__}.generate_response",
                exception_details=str(e)
            )

            response_time = time.time() - start_time
            return LLMResponse(
                content=f"调用失败: {str(e)}",
                model=model or self.default_model,
                usage={'prompt_tokens': 0, 'completion_tokens': 0, 'total_tokens': 0},
                response_time=response_time,
                provider="anthropic"
            )

    def quick_test(self) -> bool:
        """
        快速测试函数，验证LLM连接是否正常

        Returns:
            bool: 测试是否成功
        """
        try:
            test_prompt = "请简单回答：1+1等于几？"
            response = self.llm_call_with_history(test_prompt)
            success = bool(response and not response.startswith("调用失败"))

            if success:
                logger.info("✅ LLM连接测试成功！")
            else:
                logger.error(f"❌ LLM连接测试失败: {response}")

            return success

        except Exception as e:
            logger.error(f"❌ LLM连接测试失败: {str(e)}")
            return False


# 全局实例
simple_llm_service = SimpleLLMService()


def get_llm_service() -> SimpleLLMService:
    """
    获取LLM服务实例

    Returns:
        SimpleLLMService: LLM服务实例
    """
    return simple_llm_service


if __name__ == "__main__":
    # 如果直接运行此文件，执行快速测试
    print("正在测试简化Anthropic LLM连接...")
    simple_llm_service.quick_test()